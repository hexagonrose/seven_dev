{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import Callable, List, Optional\n",
    "from glob import glob\n",
    "\n",
    "from ase.io import read\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from sevenn.atom_graph_data import AtomGraphData\n",
    "from sevenn.nn.sequential import AtomGraphSequential\n",
    "from sevenn.train.dataload import unlabeled_atoms_to_graph\n",
    "import sevenn._keys as KEY\n",
    "import sevenn.util as util\n",
    "from sevenn.sevennet_calculator import SevenNetCalculator\n",
    "\n",
    "# for test\n",
    "from torch_geometric.loader import DataLoader # test\n",
    "from torch_geometric.data import Batch # batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unlabeled_graph_build(\n",
    "    atoms_list: List,\n",
    "    cutoff: float,\n",
    "    num_cores: int = 1,\n",
    "    transfer_info: bool = True,\n",
    "    y_from_calc: bool = False,\n",
    ") -> List[AtomGraphData]:\n",
    "    \"\"\"\n",
    "    parallel version of graph_build\n",
    "    build graph from atoms_list and return list of AtomGraphData\n",
    "    Args:\n",
    "        atoms_list (List): list of ASE atoms\n",
    "        cutoff (float): cutoff radius of graph\n",
    "        num_cores (int): number of cores to use\n",
    "        transfer_info (bool): if True, copy info from atoms to graph,\n",
    "                              defaults to True\n",
    "        y_from_calc (bool): Get reference y labels from calculator, defaults to False\n",
    "    Returns:\n",
    "        List[AtomGraphData]: list of AtomGraphData\n",
    "    \"\"\"\n",
    "    serial = num_cores == 1\n",
    "    inputs = [(atoms, cutoff) for atoms in atoms_list]\n",
    "\n",
    "    if not serial:\n",
    "        pool = mp.Pool(num_cores)\n",
    "        graph_list = pool.starmap(\n",
    "            unlabeled_atoms_to_graph,\n",
    "            tqdm(inputs, total=len(atoms_list), desc=f'graph_build ({num_cores})'),\n",
    "        )\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    else:\n",
    "        graph_list = [\n",
    "            unlabeled_atoms_to_graph(*input_)\n",
    "            for input_ in tqdm(inputs, desc='graph_build (1)')\n",
    "        ]\n",
    "\n",
    "    graph_list = [AtomGraphData.from_numpy_dict(g) for g in graph_list]\n",
    "\n",
    "    return graph_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading structures took 0.06 seconds.\n"
     ]
    }
   ],
   "source": [
    "material = 'Mg4Ta8O24'\n",
    "working_dir = os.getcwd()\n",
    "device='cpu'\n",
    "# Read the structure paths\n",
    "structure_paths = glob(os.path.join(working_dir, material, \"*.cif\"))\n",
    "structures = [] # List of ASE Atoms objects\n",
    "start = time.time()\n",
    "for i, structure_path in enumerate(structure_paths):\n",
    "    structure = read(structure_path)\n",
    "    structures.append(structure)\n",
    "    \n",
    "end = time.time()\n",
    "print(f\"Reading structures took {end-start:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure = structures[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc = SevenNetBatchCalculator(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure.calc = calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-314.3199768066406"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure.get_potential_energy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc.atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([12, 12, 12, 12, 73, 73, 73, 73, 73, 73, 73, 73,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8]),\n",
       " array([12, 12, 12, 12, 73, 73, 73, 73, 73, 73, 73, 73,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8]),\n",
       " array([12, 12, 12, 12, 73, 73, 73, 73, 73, 73, 73, 73,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8]),\n",
       " array([12, 12, 12, 12, 73, 73, 73, 73, 73, 73, 73, 73,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8]),\n",
       " array([12, 12, 12, 12, 73, 73, 73, 73, 73, 73, 73, 73,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from ase import Atoms\n",
    "\n",
    "class AtomsList(list):\n",
    "    \"\"\"A custom list class to hold ASE Atoms objects and add functionality.\"\"\"\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        \"\"\"Initialize the AtomsList with a list of Atoms objects.\"\"\"\n",
    "        super().__init__(*args)\n",
    "        self._validate_atoms()\n",
    "\n",
    "    def _validate_atoms(self):\n",
    "        \"\"\"Ensure all elements in the list are ASE Atoms objects.\"\"\"\n",
    "        for item in self:\n",
    "            if not isinstance(item, Atoms):\n",
    "                raise TypeError(f\"All elements must be ASE Atoms objects. Found: {type(item)}\")\n",
    "\n",
    "    def append(self, item):\n",
    "        \"\"\"Override append to validate Atoms objects.\"\"\"\n",
    "        if not isinstance(item, Atoms):\n",
    "            raise TypeError(f\"Only ASE Atoms objects can be added. Found: {type(item)}\")\n",
    "        super().append(item)\n",
    "\n",
    "    def extend(self, items):\n",
    "        \"\"\"Override extend to validate multiple Atoms objects.\"\"\"\n",
    "        for item in items:\n",
    "            if not isinstance(item, Atoms):\n",
    "                raise TypeError(f\"Only ASE Atoms objects can be added. Found: {type(item)}\")\n",
    "        super().extend(items)\n",
    "\n",
    "    def get_energies(self):\n",
    "        \"\"\"Get potential energies for all Atoms objects in the list.\"\"\"\n",
    "        return [atoms.get_potential_energy() for atoms in self]\n",
    "\n",
    "    def get_atomic_numbers(self):\n",
    "        \"\"\"Get atomic numbers for all Atoms objects.\"\"\"\n",
    "        return [atoms.get_atomic_numbers() for atoms in self]\n",
    "\n",
    "atoms_list = AtomsList(structures)\n",
    "atoms_list.get_atomic_numbers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from typing import Any, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.jit\n",
    "import torch.jit._script\n",
    "from ase.calculators.calculator import Calculator, all_changes\n",
    "from ase.data import chemical_symbols\n",
    "\n",
    "import sevenn._keys as KEY\n",
    "import sevenn.util as util\n",
    "from sevenn.atom_graph_data import AtomGraphData\n",
    "from sevenn.nn.sequential import AtomGraphSequential\n",
    "from sevenn.train.dataload import unlabeled_atoms_to_graph\n",
    "\n",
    "torch_script_type = torch.jit._script.RecursiveScriptModule\n",
    "\n",
    "\n",
    "class SevenNetBatchCalculator(Calculator):\n",
    "    \"\"\"ASE calculator for SevenNet models\n",
    "\n",
    "    Multi-GPU parallel MD is not supported for this mode.\n",
    "    Use LAMMPS for multi-GPU parallel MD.\n",
    "    This class is for convenience who want to run SevenNet models with ase.\n",
    "\n",
    "    Note than ASE calculator is designed to be interface of other programs.\n",
    "    But in this class, we simply run torch model inside ASE calculator.\n",
    "    So there is no FileIO things.\n",
    "\n",
    "    Here, free_energy = energy\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Union[str, pathlib.PurePath, AtomGraphSequential] = '7net-0',\n",
    "        file_type: str = 'checkpoint',\n",
    "        device: Union[torch.device, str] = 'auto',\n",
    "        sevennet_config: Optional[Any] = None,  # hold meta information\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Initialize the calculator\n",
    "\n",
    "        Args:\n",
    "            model (SevenNet): path to the checkpoint file, or pretrained\n",
    "            device (str, optional): Torch device to use. Defaults to \"auto\".\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.sevennet_config = None\n",
    "\n",
    "        if isinstance(model, pathlib.PurePath):\n",
    "            model = str(model)\n",
    "\n",
    "        file_type = file_type.lower()\n",
    "        if file_type not in ['checkpoint', 'torchscript', 'model_instance']:\n",
    "            raise ValueError('file_type should be checkpoint or torchscript')\n",
    "\n",
    "        if isinstance(device, str):  # TODO: do we really need this?\n",
    "            if device == 'auto':\n",
    "                self.device = torch.device(\n",
    "                    'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "                )\n",
    "            else:\n",
    "                self.device = torch.device(device)\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        if file_type == 'checkpoint' and isinstance(model, str):\n",
    "            if os.path.isfile(model):\n",
    "                checkpoint = model\n",
    "            else:\n",
    "                checkpoint = util.pretrained_name_to_path(model)\n",
    "            model_loaded, config = util.model_from_checkpoint(checkpoint)\n",
    "            model_loaded.set_is_batch_data(False)\n",
    "            self.type_map = config[KEY.TYPE_MAP]\n",
    "            self.cutoff = config[KEY.CUTOFF]\n",
    "            self.sevennet_config = config\n",
    "        elif file_type == 'torchscript' and isinstance(model, str):\n",
    "            extra_dict = {\n",
    "                'chemical_symbols_to_index': b'',\n",
    "                'cutoff': b'',\n",
    "                'num_species': b'',\n",
    "                'model_type': b'',\n",
    "                'version': b'',\n",
    "                'dtype': b'',\n",
    "                'time': b'',\n",
    "            }\n",
    "            model_loaded = torch.jit.load(\n",
    "                model, _extra_files=extra_dict, map_location=self.device\n",
    "            )\n",
    "            chem_symbols = extra_dict['chemical_symbols_to_index'].decode('utf-8')\n",
    "            sym_to_num = {sym: n for n, sym in enumerate(chemical_symbols)}\n",
    "            self.type_map = {\n",
    "                sym_to_num[sym]: i for i, sym in enumerate(chem_symbols.split())\n",
    "            }\n",
    "            self.cutoff = float(extra_dict['cutoff'].decode('utf-8'))\n",
    "        elif isinstance(model, AtomGraphSequential):\n",
    "            if model.type_map is None:\n",
    "                raise ValueError(\n",
    "                    'Model must have the type_map to be used with calculator'\n",
    "                )\n",
    "            if model.cutoff == 0.0:\n",
    "                raise ValueError('Model cutoff seems not initialized')\n",
    "            model.eval_type_map = torch.tensor(True)  # ?\n",
    "            model.set_is_batch_data(False)\n",
    "            model_loaded = model\n",
    "            self.type_map = model.type_map\n",
    "            self.cutoff = model.cutoff\n",
    "        else:\n",
    "            raise ValueError('Unexpected input combinations')\n",
    "\n",
    "        if self.sevennet_config is None and sevennet_config is not None:\n",
    "            self.sevennet_config = sevennet_config\n",
    "\n",
    "        self.model = model_loaded\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.implemented_properties = [\n",
    "            'free_energy',\n",
    "            'energy',\n",
    "            'forces',\n",
    "            'stress',\n",
    "            'energies',\n",
    "        ]\n",
    "\n",
    "    def calculate(self, atoms=None, properties=None, system_changes=all_changes):\n",
    "        # call parent class to set necessary atom attributes\n",
    "        # Calculator.calculate(self, atoms, properties, system_changes)\n",
    "        if atoms is None:\n",
    "            raise ValueError('No atoms to evaluate')\n",
    "        data = AtomGraphData.from_numpy_dict(\n",
    "            unlabeled_atoms_to_graph(atoms, self.cutoff)\n",
    "        )\n",
    "\n",
    "        data.to(self.device)  # type: ignore\n",
    "\n",
    "        if isinstance(self.model, torch_script_type):\n",
    "            data[KEY.NODE_FEATURE] = torch.tensor(\n",
    "                [self.type_map[z.item()] for z in data[KEY.NODE_FEATURE]],\n",
    "                dtype=torch.int64,\n",
    "                device=self.device,\n",
    "            )\n",
    "            data[KEY.POS].requires_grad_(True)  # backward compatibility\n",
    "            data[KEY.EDGE_VEC].requires_grad_(True)  # backward compatibility\n",
    "            data = data.to_dict()\n",
    "            del data['data_info']\n",
    "\n",
    "        output = self.model(data)\n",
    "        energy = output[KEY.PRED_TOTAL_ENERGY].detach().cpu().item()\n",
    "        # Store results\n",
    "        self.results = {\n",
    "            'free_energy': energy,\n",
    "            'energy': energy,\n",
    "            'energies': (\n",
    "                output[KEY.ATOMIC_ENERGY].detach().cpu().reshape(len(atoms)).numpy()\n",
    "            ),\n",
    "            'forces': output[KEY.PRED_FORCE].detach().cpu().numpy(),\n",
    "            'stress': np.array(\n",
    "                (-output[KEY.PRED_STRESS])\n",
    "                .detach()\n",
    "                .cpu()\n",
    "                .numpy()[[0, 1, 2, 4, 5, 3]]  # as voigt notation\n",
    "            ),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc = SevenNetBatchCalculator(device=device)\n",
    "structure.calc = calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-314.3199768066406"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure.get_potential_energy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
